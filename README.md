# Q-learning applied to OpenAI's cart-pole problem

This is my implementation of Q-learning on OpenAI's cart pole problem. Despite the state space being continuous for this problem, I discretize the values into buckets allowing Q-learning to function. 

The model fairly consistently solves the environment in about 600 timesteps.

## Example

The model learning to balance the pole with the cart

![Cart](docs/learning.gif)

## Runs

<img src="docs/Figure_1.png" width="400" />
<img src="docs/Figure_2.png" width="400" />
<img src="docs/Figure_3.png" width="400" />

